{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rtKB6RM67q3a",
    "outputId": "58622962-10d3-4115-b000-95ab4a503bbd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dJFvbuMnhRmb",
    "outputId": "34ee34da-b6f9-45c0-9ed0-7982357c8983"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "#%cd /content/drive/MyDrive/Char_Transf_DataSet\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "id": "yneQTDEA2Uiq"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "block_size=32\n",
    "n_embed=64\n",
    "batch_size=16\n",
    "max_iters=200000\n",
    "learning_rate=1e-6\n",
    "eval_iters=200\n",
    "eval_interval=10000\n",
    "device='cuda' if torch.cuda.is_available() else 'cpu'\n",
    "n_heads=4\n",
    "n_layers=4\n",
    "dropout=0.0\n",
    "\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "# wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
    "#path='C:/Users/adith/OneDrive/Pictures/deletable git tut/deletable tiny shake test/input.txt'\n",
    "\n",
    "path = '/content/drive/MyDrive/Char_Transf_DataSet/input.txt'\n",
    "\n",
    "with open(path, 'r', encoding='utf-8') as f:\n",
    "    data = f.read()\n",
    "\n",
    "vocab=sorted(list(set(data)))\n",
    "vocab_size=len(vocab)\n",
    "chtoi={ch:i for i,ch in enumerate(vocab)}\n",
    "itoch={i:ch for i,ch in enumerate(vocab)}\n",
    "\n",
    "encode=lambda s:[chtoi[c] for c in s]\n",
    "decode=lambda l:''.join([itoch[int(i)] for i in l])\n",
    "\n",
    "data=encode(data)\n",
    "\n",
    "data=torch.tensor(data,dtype=torch.long)\n",
    "\n",
    "n=int(0.9*len(data))\n",
    "train_data=data[:n]\n",
    "eval_data=data[n:]\n",
    "\n",
    "#lprint function if needed goes here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_batch(split):\n",
    "    data= train_data if split=='train' else eval_data\n",
    "    idx=torch.randint(len(data)-block_size,(batch_size,))\n",
    "    inp=torch.stack([data[i:i+block_size] for i in idx])  #a list of tensors\n",
    "    out=torch.stack([data[i+1:i+1+block_size] for i in idx])\n",
    "    inp,out=inp.to(device),out.to(device)\n",
    "    if inp.shape!=(16,32) or out.shape!=(16,32):\n",
    "        print('get batch, inp.shape', inp.shape)\n",
    "        print('get batch, out.shape', out.shape)\n",
    "        sys.exit()\n",
    "    return inp,out\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    model.eval()\n",
    "    dic={}\n",
    "    for split in ['train','eval']:\n",
    "        losses=torch.zeros((eval_iters,),dtype=torch.float64)\n",
    "        for k in range(eval_iters):\n",
    "            x,y=get_batch(split)\n",
    "            logits,loss=model(x,y)\n",
    "            losses[k]=loss.item()\n",
    "\n",
    "\n",
    "        dic[split]=losses.mean()\n",
    "    print(dic)\n",
    "    model.train()\n",
    "    return None\n",
    "\n",
    "\n",
    "class Head(nn.Module):\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key=nn.Linear(n_embed,head_size,bias=False)\n",
    "        self.query=nn.Linear(n_embed,head_size,bias=False)\n",
    "        self.value=nn.Linear(n_embed,head_size,bias=False)\n",
    "        self.register_buffer('tril',torch.tril(torch.ones(block_size,block_size)))\n",
    "        self.dropout=nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self,x):\n",
    "\n",
    "        B,T,C=x.shape\n",
    "        k=self.key(x)\n",
    "        q=self.query(x)\n",
    "        wei=q@k.transpose(-2,-1)*C**-0.5\n",
    "        if self.tril.shape!=(32,32):\n",
    "            print('wei',wei,wei.shape)\n",
    "            print('tril',self.tril,self.tril.shape)\n",
    "        B1,T1,T2=wei.shape\n",
    "        if (T1,T2)!=(32,32):\n",
    "            print('wei',wei,wei.shape)\n",
    "            print('tril',self.tril,self.tril.shape)\n",
    "            print('x shape',x.shape)\n",
    "            print('q shape',q.shape)\n",
    "            print('k shape',k.shape)\n",
    "        wei.masked_fill(self.tril==0,float('-inf'))\n",
    "        wei=F.softmax(wei,dim=-1)\n",
    "        wei=self.dropout(wei)\n",
    "        v=self.value(x)\n",
    "        return wei@v\n",
    "\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, n_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads=nn.ModuleList([Head(head_size) for _ in range(n_heads)])\n",
    "        self.proj=nn.Linear(n_embed,n_embed)\n",
    "        self.dropout=nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self,x):\n",
    "\n",
    "        out=[h(x) for h in self.heads]\n",
    "        out=torch.cat(out,dim=-1)\n",
    "        out=self.proj(out)\n",
    "        return self.dropout(out)\n",
    "\n",
    "\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.run=nn.Sequential(nn.Linear(n_embed,n_embed*4),\n",
    "                              nn.ReLU(),\n",
    "                              nn.Linear(n_embed*4,n_embed),\n",
    "                              nn.Dropout(dropout))\n",
    "    def forward(self,x):\n",
    "        return self.run(x)\n",
    "\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, n_embed, n_heads):\n",
    "        super().__init__()\n",
    "        head_size=n_embed//n_heads\n",
    "        self.run=nn.Sequential(nn.LayerNorm(n_embed),\n",
    "                               MultiHeadAttention(n_heads,head_size))\n",
    "        self.run2=nn.Sequential(nn.LayerNorm(n_embed),\n",
    "                               FeedForward())\n",
    "    def forward(self,x):\n",
    "        x=x+self.run(x)\n",
    "        x=x+self.run2(x)\n",
    "        return x\n",
    "\n",
    "class lang_model(nn.Module):\n",
    "    def __init__(self,n_embed,n_heads):\n",
    "        super().__init__()\n",
    "        self.TET=nn.Embedding(vocab_size,n_embed)\n",
    "        self.PET=nn.Embedding(block_size,n_embed)\n",
    "        self.run=nn.Sequential(*[Block(n_embed,n_heads) for _ in range(n_layers)])\n",
    "        self.run2=nn.Sequential(nn.LayerNorm(n_embed),\n",
    "                               nn.Linear(n_embed,vocab_size))\n",
    "    def forward(self,x,y=None):\n",
    "        B,T=x.shape\n",
    "\n",
    "        out=self.TET(x)+self.PET(torch.arange(T,device=device))\n",
    "\n",
    "        out=self.run(out)\n",
    "        logits=self.run2(out)\n",
    "        B,T,C = logits.shape\n",
    "        logits=logits.view(B*T,C)#512,94 shape n_of_examples,num_of_classes, class props have to be unnornmalised\n",
    "\n",
    "        #print('logits pre cross entro',logits)\n",
    "        #print('y.flatten',y.view(-1))\n",
    "        #print('@forward y is', decode(y.view(-1).to(torch.int))) if y is not None else None\n",
    "        loss=F.cross_entropy(logits,y.view(-1)) if y is not None else None\n",
    "        #print('loss',loss,loss.shape)\n",
    "        return logits,loss\n",
    "\n",
    "    def generate(self,x=torch.zeros((16,32),dtype=torch.long,device=device),num_tokens=100):\n",
    "\n",
    "        for _ in range(num_tokens):\n",
    "                consider=x[:,-32:]#this is legal even for small tensors\n",
    "                logit,_=m(consider)#shouldnt this be self(consider) and not m(consider)?\n",
    "                logit=torch.softmax(logit,dim=-1)#take the softmax of the B*T,C tensor across C get=B*T,C\n",
    "                logit=torch.multinomial(logit,num_samples=1) #get B*T,1\n",
    "                logit=logit.view(16,-1) #get B,T\n",
    "                logit=logit[:,-1:] #get B,1.....we do this to get only the last of the logit, the TRUE predicted excess.\n",
    "                x=torch.cat([x,logit],dim=-1)\n",
    "        return decode(x[0].view(-1).to(torch.int))\n",
    "    def beam_search(self,x=torch.zeros((16,32),dtype=torch.long,device=device),num_tokens=100,beam_width=3):\n",
    "\n",
    "\n",
    "      class next_i_creator():\n",
    "        def __init__(self):\n",
    "          self._i=0\n",
    "        def __call__(self):\n",
    "          self._i=self._i+1\n",
    "          return str(self._i)\n",
    "\n",
    "      def grow(x):#takes a sequence and gives list extendedSeq,score\n",
    "        #t is tensor of integers [:94]\n",
    "        #turn tensor of rank1 to rank2.shape(16,X)\n",
    "        #maintain window size\n",
    "\n",
    "        x=torch.tensor(x)\n",
    "        x=torch.stack([x]*16)#1D to 2D\n",
    "\n",
    "        consider=x[:,-32:]# 2D of the right shape\n",
    "\n",
    "        logit,_=m(consider)#get index,probability as list\n",
    "        logit=torch.softmax(logit,dim=-1)#512,94\n",
    "        probs=logit[:1,:]#1,94 get only the first logit...\n",
    "        logit=torch.multinomial(logit,num_samples=beam_width)[:1,:]#512,3[:1,:]->1,3\n",
    "        ret_prob=logit.clone().to(torch.float32).apply_(lambda x: probs[0,int(x)])\n",
    "        return list(zip(logit.flatten(),ret_prob.flatten()))# list of j,ss vocabInteger,probabilityScore\n",
    "\n",
    "      ni=next_i_creator()\n",
    "      pool=[[ni(),x[0],1]]\n",
    "      temp_pool=[]\n",
    "\n",
    "      for _ in range(num_tokens):\n",
    "        #grow pool\n",
    "        temp_pool=[]\n",
    "        for i,t,s in pool:\n",
    "\n",
    "          temp_pool.extend([[ni(),torch.cat((t,j.unsqueeze(0)),dim=0),s*ss] for j,ss in grow(t)])#i,t,s are index,sequence,s is score of seq, ss is score of new tok\n",
    "\n",
    "        #score and prune pool\n",
    "        pool=sorted(temp_pool,key=lambda x:x[2],reverse=True)[:beam_width]#3,3 3sequences,3cols.indx,seq,score\n",
    "      return pool #returns list index,sequence,score\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yIUSU51UYpR_"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "id": "UAJk3J9L_UtE",
    "outputId": "e5d53549-1736-48c4-fdb3-2e67c2c763c8"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "\"model=lang_model(n_embed,n_heads)\\nm=model.to(device)\\noptimizer=torch.optim.AdamW(m.parameters(),lr=learning_rate)\\ntorch.save(model.state_dict(), '/content/drive/MyDrive/Char_Transf_DataSet/ADI_Transformer.pth')\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if not os.path.exists('/content/drive/MyDrive/Char_Transf_DataSet/ADI_Transformer.pth'):\n",
    "  model=lang_model(n_embed,n_heads)\n",
    "  m=model.to(device)\n",
    "  optimizer=torch.optim.AdamW(m.parameters(),lr=learning_rate)\n",
    "  torch.save(model.state_dict(), '/content/drive/MyDrive/Char_Transf_DataSet/ADI_Transformer.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vb5i6XLH-zhy",
    "outputId": "c1eba11d-ad0b-4150-aa47-373219bf51d2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "213470.0  parameters\n",
      "device name: cpu\n",
      "iter is【 0 】 {'train': tensor(0.0850, dtype=torch.float64), 'eval': tensor(0.0867, dtype=torch.float64)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-64-13118b33cb3e>:213: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x=torch.tensor(x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                                                                    \n"
     ]
    }
   ],
   "source": [
    "#model=lang_model(n_embed,n_heads)\n",
    "model.load_state_dict(torch.load('/content/drive/MyDrive/Char_Transf_DataSet/ADI_Transformer.pth'))\n",
    "m=model.to(device)\n",
    "optimizer=torch.optim.AdamW(m.parameters(),lr=learning_rate)\n",
    "max_iters=1000\n",
    "print(sum(p.numel() for p in m.parameters())/1, ' parameters')\n",
    "print('device name:',device)\n",
    "for i in range(max_iters):\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    x,y=get_batch('train')\n",
    "    if i%eval_interval==0:\n",
    "        print('iter is【',i,'】 ',end='')\n",
    "        estimate_loss()\n",
    "        g=m.beam_search()\n",
    "        print(decode(g[0][1]))\n",
    "    logits,loss=m(x,y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "torch.save(model.state_dict(), '/content/drive/MyDrive/Char_Transf_DataSet/ADI_Transformer.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WGiw4y0v7iZG",
    "outputId": "ba3406c7-321c-443e-f2cf-1c2a04ff5af4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-64-13118b33cb3e>:213: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x=torch.tensor(x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "over the tuesday weather the makeshift reporter was seenthe mayeshift reporter was seenthe mayeshift reporter was seenthe mayeshift reporter was seenthe may\n"
     ]
    }
   ],
   "source": [
    "model=lang_model(n_embed,n_heads)\n",
    "model.load_state_dict(torch.load('/content/drive/MyDrive/Char_Transf_DataSet/ADI_Transformer.pth'))\n",
    "m=model.to(device)\n",
    "x='over the tuesday weather the makeshift reporter was seen'\n",
    "x=torch.tensor(encode(x),dtype=torch.int)\n",
    "x=torch.stack([x]*16)# 16 rows and x.len coloumns\n",
    "#print('x in trying post enc.stack',x)\n",
    "\n",
    "\n",
    "g=m.beam_search(x=x,num_tokens=100,beam_width=3)#decode(x[0].view(-1))\n",
    "g=g[1][1]#give the integers.sequence I think\n",
    "print(decode(g))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "cUuKjAElwy6g"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EpcEGD0q8Eg0",
    "outputId": "6e2ee601-7aae-4734-a1ad-cd142f5a69c0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "on tuesday the honourable prime minister has made for another y ackie.\\nRe s.\\nVur barablion wharofalitasa Iadericans siis or Rercinyc thin HH Uonam saprngG.%500\n"
     ]
    }
   ],
   "source": [
    "model=lang_model(n_embed,n_heads)\n",
    "model.load_state_dict(torch.load('/content/drive/MyDrive/Char_Transf_DataSet/ADI_Transformer.pth'))\n",
    "m=model.to(device)\n",
    "x='on tuesday the honourable prime minister has made for another'\n",
    "x=torch.tensor(encode(x),dtype=torch.int)\n",
    "x=torch.stack([x]*16)# 16 rows and x.len coloumns\n",
    "#print('x in trying post enc.stack',x)\n",
    "\n",
    "g=m.generate(x=x,num_tokens=100)#WITHOUT BEAM SEARCH, OUTPUT MAKES NO SENSE\n",
    "print(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
